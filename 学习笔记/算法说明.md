# BasicModels

## 线性回归（Linear Regression）

回归目的是使预测值尽可能接近真实值。

线性回归指的是给定输入的特征向量 x，学习一组参数 w、b，使用线性计算得到预测值 y'，即 $y'=w\times x+b$，使 y' 尽可能接近真实值 y。

线性回归分为 2 类：

- 一元线性回归：`y=w*x+b`，`x.shape=(1,)`，`y.shape=(1, )`，x 和 y 一一对应，计算标量 w 值和标量 b 值
- 多元线性回归：`y=w[0]*x[0]+w[1]*x[1]+w[2]*x[2]+...+w[n-1]*x[n]+b`，`x.shape=(n,)`，`y.shape=(1, )`，n个输入值组成的向量 x 对应 1 个 y 值，计算向量 w 和标量 b 值

损失函数使用最小二乘法：
$$
c = \frac{\sum_1^{m} (y'-y)^2}{2m}
$$
上述公式中样本数为 m

使用梯度下降法进行优化

一元线性回归代码如下：

```python
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

# 定义参数
learning_rate = 0.01
training_epochs = 500

# 建立训练数据
x = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1], dtype=np.float32)
y = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,2.827,3.465,1.65,2.904,2.42,2.94,1.3], dtype=np.float32)

# 建立tf.placeholder
X = tf.placeholder(dtype=tf.float32)  # 默认shape参数值为None，表示shape完全由使用时的feed_dict参数决定
Y = tf.placeholder(dtype=tf.float32)  # 默认shape参数值为None，表示shape完全由使用时的feed_dict参数决定

# 建立tf变量
w = tf.Variable(np.random.randn(), name="weight")  # 标量
b = tf.Variable(np.random.randn(), name="bias")  # 标量

# 计算结果
Y_ = tf.add(tf.multiply(X, w), b)  # multiply和add计算支持broadcasting，使得标量w/a与X的每个元素相乘/相加

# 损失函数
cost = tf.reduce_sum(tf.pow((Y_-Y), 2))/x.shape[0]  # 用除以x.shape[0]实现归一化

# 定义优化器
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# 训练
with tf.Session() as sess:
	# 变量初始化
	sess.run(tf.global_variables_initializer())

	# 训练循环
	# 2种方法：1次优化使用全部训练数据；1次优化使用训练数据中的一组
	for i in range(training_epochs):
		# 训练方法1：
		# feed_dict的向shape为None的placeholder送入数据，placeholder的shape即为参数shape
		sess.run(optimizer, feed_dict={X: x, Y: y})
		# 训练方法2：
		# for xx, yy in zip(x, y):
		# 	sess.run(optimizer, feed_dict={X: xx, Y: yy})

	print("finished training")

	# 显示图像
	plt.plot(x, y, "ro", label="training data")
	plt.plot(x, sess.run(w)*x+sess.run(b), label="fitted")
	plt.legend()
	plt.show()

```

## 逻辑回归（Logistic Regression）

逻辑回归用于解决二分类问题，需要一个映射函数，将分类结果映射成为 [0, 1] 之间的概率值，并且该函数具有很好的可微分性。

在二分类问题上，sigmoid 函数满足上述条件：
$$
y'=g(z)=\frac{1}{1+e^{-z}}
$$
仿照线性回归公式，z值计算方法：`z=w[0]*x[0]+w[1]*x[1]+w[2]*x[2]+...+w[n-1]*x[n-1]+b`

对于 m 种类别的分类，每个类别都计算得到对应 1 个 z 值，即对应 1 个预测值 y'

设样本输入值 x.shape=(None, n)：None 维度表示样本数目，n 表示每个样本的数值个数，则有

样本输出值 y.shape=(None, m)

权重 w.shape=(n, m)，偏置 b.shape=(m, )，预测值 y'.shape=(None, m)

损失函数使用交叉熵（Cross Entropy）：
$$
c=-\sum_{i=0}^{i=m-1} y_i\times log(y'_i)
$$
一般情况下也可以将交叉熵除以分类数目 m，实现计算结果归一化。

在二分类的情况下，m 值为 2，则有样本输出值 y 只有 0 或者 1 两种情况，sigmoid 输出 y' 值使用标量即可以表示其分类概率，因此二分类的特殊情况下损失函数交叉熵：
$$
c=-y\times log(y')-(1-y)\times log(1-y')
$$
使用梯度下降法进行优化

## Softmax 回归

Softmax 回归用于解决多分类问题，用 [0, 1] 之间的概率值表示当前样本属于某个分类的可能性，使用 Softmax 函数计算类别 i 在 m 个分类中的可能性：
$$
y'_i=\frac{e^{z_i}}{\sum_{j=0}^{j=m-1}e^{z_j}}
$$
z值计算方法：`z=w[0]*x[0]+w[1]*x[1]+w[2]*x[2]+...+w[n-1]*x[n-1]+b`

与逻辑回归一样，对于 m 种类别的分类，每个类别都计算得到对应 1 个 $z_i$ 值，即对应 1 个预测值 $y'_i$

设样本输入值 x.shape=(None, n)：None 维度表示样本数目，n 表示每个样本的数值个数，则有

样本输出值 y.shape=(None, m)：y 为独热码表示，即正确类别的位置处为 1 值，其它位置全为 0 值

权重 w.shape=(n, m)，偏置 b.shape=(m, )，预测值 y'.shape=(None, m)

损失函数使用交叉熵（Cross Entropy）：
$$
c=-\sum_{i=0}^{i=m-1} y_i\times log(y'_i)
$$
一般情况下也可以将交叉熵除以分类数目 m，实现计算结果归一化。

使用梯度下降法优化

Softmax 回归算法训练 mnist 数据集代码如下：

```python
import tensorflow as tf
import random
import numpy as np
import matplotlib.pyplot as plt

# 导入mnist数据集
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("d:/temp/mnist", one_hot=True)

# 从mnist数据集中取出训练数据
train_images = mnist.train.images
train_labels = mnist.train.labels

# 从mnist数据集中取出测试数据
test_images = mnist.test.images
test_labels = mnist.test.labels

# 显示前10幅图像
# for i in range(10):
# 	plt.ion()  # 打开交互模式
# 	plt.imshow(train_images[i].reshape((28, 28), order="C"), cmap='gray')  # 用ndarray.reshape改变shape为实际图像
# 	label_show = train_labels[i]  # 当前label
# 	label_tuple = np.where(label_show == np.max(label_show))  # 用np.where取得最大值的位置即为当前label值，格式为tuple
# 	plt.title("label: {}".format(label_tuple[0]))  # np.where返回tuple[0]为array，数值为满足条件元素的索引
# 	plt.show()
# 	plt.pause(1)  # 等待1秒
# 	plt.clf()  # 清除当前图像

# 定义训练参数
learning_rate = 0.01
training_epochs = 50000
batch_size = 100

# 建立placeholder
x = tf.placeholder(tf.float32, (None, 784))  # 用于装入batch_size个训练图像数据
y = tf.placeholder(tf.float32, (None, 10))  # 对应batch_size个训练图像的label

# 定义权重和偏置变量
# 用正态分布随机数作为初值，收敛速度较慢
# w = tf.Variable(tf.random_normal((784, 10), dtype=tf.float32))
# b = tf.Variable(tf.random_normal((10,), dtype=tf.float32))
# 用0初值，收敛速度快
w = tf.Variable(tf.zeros((784, 10), dtype=tf.float32))
b = tf.Variable(tf.zeros((10,), dtype=tf.float32))

# 计算softmax作为输出值
y_ = tf.nn.softmax(tf.add(tf.matmul(x, w), b))

# 交叉熵作为损失函数
# axis=1的reduce_sum用于计算每个样本输出softmax中10个元素的和值
# 最外层的reduce_mean用于计算全部样本的均值
cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_), axis=1))

# 定义优化器
optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

# 训练
with tf.Session() as sess:
	# 变量初始化
	sess.run(tf.global_variables_initializer())

	# 取得全部训练样本的索引列表
	image_index_list = list(range(train_images.shape[0]))

	# 训练循环
	for i in range(training_epochs):
		# 从训练样本中随机取出batch_size个图像用于此轮训练
		image_batch_index = random.sample(image_index_list, batch_size)  # 从全部索引列表中取出batch_size个索引值
		batch_images = train_images[image_batch_index]  # 从训练样本中取得batch_size个图片
		batch_labels = train_labels[image_batch_index]  # 从训练样本中取得batch_size个label

		# 计算损失值和优化器
		c, _ = sess.run([cost, optimizer], feed_dict={x: batch_images, y: batch_labels})

		# 打印当前训练结果
		if (i+1) % 100 == 0:
			print("epoch: {}\tcost: {:.2f}".format(i+1, c))

		# 每10000次，learning_rate变为0.5倍
		if (i+1) % 10000 == 0:
			learning_rate *= 0.5

	# 显示训练结果
	for i in range(100):
		ind = random.randint(0, test_images.shape[0]-1)  # 取得待测试图像的随机索引
		image = test_images[ind]  # 随机取出1幅图像
		plt.ion()  # 打开交互模式
		plt.imshow(image.reshape((28, 28), order="C"), cmap='gray')  # 用ndarray.reshape改变shape为实际图像

		# 计算label
		label = sess.run(y_, feed_dict={x: image.reshape(1, 784)})  # 用训练后的变量计算softmax
		label = label[0]  # sess.run返回label的shape为(1, 10)，为了用于查找最大值，取出label[0]
		label_pred = np.where(label == np.max(label))  # 用np.where取得最大值的位置即为当前label值，格式为tuple

		# 正确label
		label = test_labels[ind]
		label_rite = np.where(label == np.max(label))  # 用np.where取得最大值的位置即为当前label值，格式为tuple

		# 根据计算是否正确显示不同title
		if label_pred[0][0] == label_rite[0][0]:
			plt.title("rite label: {}".format(label_pred[0][0]))
		else:
			plt.title("pred: {}\trite: {}".format(label_pred[0][0], label_rite[0][0]))

		plt.show()
		plt.pause(1)  # 等待1秒
		plt.clf()  # 清除当前图像

```

